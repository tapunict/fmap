{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9211c334",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>fmap (Frequence Mapper)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87453b56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"right\" src=\"./assets/cover.png\" style=\"width: 70%; left:30px\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccca0b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduzione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f385bd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si vuole costruire un sistema di monitoraggio real-time (o near-real-time) del segnale delle stazioni radio nelle provincie siciliane. Si vuole visualizzare graficamente l'andamento del segnale ed, eventualmente, effettuare una analisi di regressione atta a predire il comportamento futuro del segnale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4ceb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc61ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"./assets/data-flow.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6dee9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962eadf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In ogni provincia siciliana verrà piazzato un microcontrollore, prototipato attraverso una board Arduino MEGA 2560 a cui viene aggiunto un modulo SparkFun FM Tuner Evaluation Board Si4703."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32f5b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "    <tr style=\"text-align: center\">\n",
    "        <td><img src=\"./assets/arduino_mega.jpg\" style=\"width: 50%\"></td>\n",
    "        <td><img src=\"./assets/si4703.jpg\" style=\"width: 50%\"></td>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4ecda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Il modulo cattura i pacchetti RDS (Radio Data System) inviati dalle stazioni radio. Il pacchetto contiene informazioni interessanti:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df97ea4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **PI (Programme Identifier)**: identifica univocamente una stazione radio con 4 caratteri esadecimali;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fc793",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **RSSI (Received Signal Strength Indication)** un numero che va da -120 a 0  ed indica la qualità del segnale (che migliora con l'avvicinarsi allo 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0e0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Il modulo cambia periodicamente frequenza e tenta di raccogliere pacchetti RDS. In caso di successo, scrive in un file di log i seguenti dati: \n",
    "\n",
    "- RSSI; \n",
    "- frequenza;\n",
    "- PI (anche se quest'ultimo può mancare nel caso di un segnale particolarmente debole)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc8ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Per poter leggere i dati da Arduino è stato realizzato uno script Python, il quale sfrutta la porta seriale per ottnere i dati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e098e54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### arduino/serial_reading.py\n",
    "\n",
    "```python\n",
    "import serial\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ser = serial.Serial('/dev/ttyACM0', 115200, timeout = 1000)\n",
    "ser.flushInput()\n",
    "\n",
    "current_date = datetime.now().strftime('%d-%m-%Y')\n",
    "filename = f'../logs/arduino-{current_date}.log'\n",
    "f = open(filename, \"w\")\n",
    "f.close() \n",
    "\n",
    "while True:\n",
    "    ser_bytes = ser.readline().decode(\"utf-8\")[:-2] + \"\\n\"\n",
    "    with open(filename,\"a\") as f:\n",
    "        f.write(ser_bytes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee18e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/meme_pi.jpg\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efce00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "È stato, inoltre, realizzato un ulteriore script Python in grado di simulare il funzionamento di un microcontrollore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f96143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### simulator/app.py\n",
    "\n",
    "```python\n",
    "def microcontroller(province): \n",
    "    logpath = generate_path_by_province(province)\n",
    "    while(True):\n",
    "        with open(logpath, 'a') as log:\n",
    "            data  = f'province={province} '\n",
    "            data += f'coords={province_coords.get(province)} '\n",
    "            data += f'FM={generate_random_frequence_by_province(province)} '\n",
    "            data += f'RSSI={random.randint(-120,0)} \\n'            \n",
    "            log.write(data)\n",
    "            time.sleep(.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b398ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a904c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./assets/logstash-cover.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc528514",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logstash è un progetto open-source per l'analisi dei log in tempo reale. Utilizziamo una instanza di Logstash per ogni microcontrollore, che si occupi di trasportare i dati in un canale centralizzato (data ingestion). Utilizzare un canale differente per ogni microcontrollore permette di aumentare la fault tolerance del sistema: nel caso in cui una istanza di logstash non funzioni più, le altre istanze non ne risentirebbero e continuerebbero a lavorare. Logstash trasporterà i dati su Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb571060",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data streaming e Data enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b151d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosa è Kafka? \n",
    "\n",
    "Secondo RedHat: \"*Apache Kafka è una piattaforma per il data streaming distribuita che  permette di pubblicare, sottoscrivere, archiviare ed elaborare flussi di record in tempo reale. È progettata per gestire flussi di dati  provenienti da più fonti distribuendoli a più consumatori. In breve,  consente di spostare grandi quantità di dati da un punto qualsiasi a un  altro nello stesso momento.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257d4df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosa è Kafka Stream?\n",
    "\n",
    "Kafka Stream è una libreria client per sviluppare applicazioni e microservizi, dove i dati di input e di output sono conservati in un cluster Kafka. Tali applicazioni gestiscono dati in tempo reale. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5803961",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/kafka-stream-schema.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d129eae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosa è stato fatto?\n",
    "\n",
    "Utilizzando Kafka Stream, abbiamo sviluppato un'applicazione Java che, consultando il PI o la frequenza del segnale inviato, arricchisce il dato aggiungendo il nome della stazione. Quest'ultimo si ottiene da un lookup su delle tabelle con corrispondenze Frequenza-Stazione o PI-Stazione, estratte attraverso un processo di scraping su siti autorevoli. Il risultato viene insierito su un ulteriore topic, chiamato `rds-signal-output`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb084c0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/kafka-stream-uml.jpg\" width=\"70%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d344be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/meme_kafka_streaming.jpg\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba011f06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87a3f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Due consumatori consumano lo streaming dallo stesso topic (rds-signal-output), ma con due group-id differenti. Essi sono:  \n",
    "\n",
    "* Python app (trasporto dei dati su ES);\n",
    "* Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11e2af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d72bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Il primo consumatore è un semplice script in python che instanzia un **Kafka consumer** e redirige i dati su Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cde7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### kafka-to-es/connect.py\n",
    "\n",
    "```python\n",
    "kconsumer = KafkaConsumer('rds-signal-output',\n",
    "    client_id='kafka-to-es-consumer', \n",
    "    group_id ='kafka-to-es', \n",
    "    bootstrap_servers=['kafkaserver:9092'],\n",
    "    value_deserializer=json_deserializer)\n",
    "\n",
    "elasticsearch = Elasticsearch([{'host':'elasticsearch', 'port': 9200}])\n",
    "create_es_index_mapping(elasticsearch)\n",
    "\n",
    "for message in kconsumer:\n",
    "    print('sending message to es.')\n",
    "    message = correct_message_format(message.value)\n",
    "    ingest_msg_to_elasticsearch(message, elasticsearch)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b43ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ma... Cosa è Elasticsearch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ec2eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./assets/elasticsearch_cover.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a79cae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Elasticsearch è un motore di ricerca e analisi distribuito (open source) per tutti i tipi di dati, inclusi testuali, numerici, geospaziali,  strutturati e non strutturati. Conserviamo i nostri dati all'interno di Elasticsearch poiché, in combinazione con Kibana, ci permette di eseguire le analisi proposte nei requisiti del progetto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a365d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data processing e machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdd1dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./assets/apache_spark_cover.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3011e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il secondo consumatore è Spark, ovvero una piattaforma open source per l’elaborazione di analisi dei dati su larga scala, progettata per essere veloce e generica. Nello specifico, utilizziamo **Spark Structured Streaming**, che ci permette di lavorare su dati in tempo reale raggruppati all'interno di un'astrazione ad alto livello, il dataframe. Un dataframe è come una tabella contenente i dati e su cui possono essere svolte delle operazioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068ee02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Attraverso Spark structured streaming otteniamo un dataframe (in streaming) contenente i dati trasmessi in tempo reale da Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d4be5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def get_rds_signal_stream(schema: StructType):\n",
    "    return spark.readStream \\\n",
    "        .format('kafka') \\\n",
    "        .option('kafka.bootstrap.servers', 'kafkaserver:9092') \\\n",
    "        .option('subscribe', 'rds-signal-output') \\\n",
    "        .option(\"kafka.group.id\", \"spark-consumer\") \\\n",
    "        .load() \\\n",
    "        .select('timestamp', 'value') \\\n",
    "        .withColumn('time', to_timestamp('timestamp', 'YYYY/MM/DD hh:mm:ss')) \\\n",
    "        .withColumn('json_content', col('value').cast('string')) \\\n",
    "        .withColumn('content', from_json(col('json_content'), schema)) \\\n",
    "        .select(col('time'), col('content.*')) \\\n",
    "        .withColumn('milliseconds', unix_timestamp('@timestamp', format=\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \\\n",
    "        .select(\n",
    "            col('time'), \n",
    "            col('station_name'), \n",
    "            col('RSSI').cast('long'), \n",
    "            col('@timestamp'), \n",
    "            col('province'), \n",
    "            col('FM'), \n",
    "            col('coords'),\n",
    "            col('milliseconds')\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368cc4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Raggruppiamo tali dati per provincia, per stazione e per una finestra temporale di 1 minuto. Ciò implica che il dataframe è diviso in tanti gruppi, dove ogni gruppo contiene dati in una determinata finestra temporale, appartenenti ad una sola provincia ed emessi da una sola stazione radio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430f0e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "win = window(signalStream.time, \"1 minutes\")\n",
    "signalStream \\\n",
    "    .groupBy('province', 'station_name', win) \\\n",
    "    .applyInPandas(predict, get_resulting_df_schema()) \\\n",
    "    .writeStream \\\n",
    "    .option('checkpointLocation', '/save/location') \\\n",
    "    .format('es') \\\n",
    "    .start('rds-signal-output-spark') \\\n",
    "    .awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83646940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ad ogni gruppo applichiamo una funzione che esegue una analisi di regressione lineare, attraverso **scikit-learn**, in cui la variabile indipendente è il timestamp del segnale, mentre la variabile dipendente è l'RSSI. Una volta trovata la migliore retta di approssimazione, si prevede l'RSSI per i 5 minuti successivi. L'output di Spark sarà proiettato nel futuro e cercherà di predire la forza del segnale emesso dalle stazioni. I dati predetti sono depositati su elasticsearch in un indice chiamato \"rds-signal-output-spark\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a685c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"./assets/scikit_learn_cover.png\" width=\"30%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7921531",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58001c79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kibana è l'anello mancante..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27ed85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./assets/kibana_cover.png\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb431e0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "È un’interfaccia web estensibile per la presentazione visiva dei dati raccolti. Insieme ad Elasticsearch e allo strumento di elaborazione Logstash forma il cosiddetto stack ELK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f7b29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Attraverso Kibana aggreghiamo i dati dei microcontrollori dall'indice \"rds-signal-output\" con i dati predetti da Spark in \"rds-signal-output-spark\" in un unico pattern \"rds-signal-output*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4233a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Creiamo delle dashboard che ci permettono di visualizzare l'andamento dei dati, la predizione e altre preziose informazioni, il tutto in tempo reale. Vediamo qualche grafico d'esempio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75238cfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/dashboards.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd43022",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./assets/dashboards_01.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc3e6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\"><center><img src=\"./assets/group.png\" width=\"40%\"></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">\n",
    "            Coded by <a href=\"https://github.com/LemuelPuglisi\">Lemuel Puglisi</a> e <a href=\"https://github.com/Gigi-G\">Luigi Seminara</a>.<br>\n",
    "            <center><a href=\"https://github.com/triglie/fmap\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij48cGF0aCBkPSJNMTIgMGMtNi42MjYgMC0xMiA1LjM3My0xMiAxMiAwIDUuMzAyIDMuNDM4IDkuOCA4LjIwNyAxMS4zODcuNTk5LjExMS43OTMtLjI2MS43OTMtLjU3N3YtMi4yMzRjLTMuMzM4LjcyNi00LjAzMy0xLjQxNi00LjAzMy0xLjQxNi0uNTQ2LTEuMzg3LTEuMzMzLTEuNzU2LTEuMzMzLTEuNzU2LTEuMDg5LS43NDUuMDgzLS43MjkuMDgzLS43MjkgMS4yMDUuMDg0IDEuODM5IDEuMjM3IDEuODM5IDEuMjM3IDEuMDcgMS44MzQgMi44MDcgMS4zMDQgMy40OTIuOTk3LjEwNy0uNzc1LjQxOC0xLjMwNS43NjItMS42MDQtMi42NjUtLjMwNS01LjQ2Ny0xLjMzNC01LjQ2Ny01LjkzMSAwLTEuMzExLjQ2OS0yLjM4MSAxLjIzNi0zLjIyMS0uMTI0LS4zMDMtLjUzNS0xLjUyNC4xMTctMy4xNzYgMCAwIDEuMDA4LS4zMjIgMy4zMDEgMS4yMy45NTctLjI2NiAxLjk4My0uMzk5IDMuMDAzLS40MDQgMS4wMi4wMDUgMi4wNDcuMTM4IDMuMDA2LjQwNCAyLjI5MS0xLjU1MiAzLjI5Ny0xLjIzIDMuMjk3LTEuMjMuNjUzIDEuNjUzLjI0MiAyLjg3NC4xMTggMy4xNzYuNzcuODQgMS4yMzUgMS45MTEgMS4yMzUgMy4yMjEgMCA0LjYwOS0yLjgwNyA1LjYyNC01LjQ3OSA1LjkyMS40My4zNzIuODIzIDEuMTAyLjgyMyAyLjIyMnYzLjI5M2MwIC4zMTkuMTkyLjY5NC44MDEuNTc2IDQuNzY1LTEuNTg5IDguMTk5LTYuMDg2IDguMTk5LTExLjM4NiAwLTYuNjI3LTUuMzczLTEyLTEyLTEyeiIvPjwvc3ZnPg==\"></a></center>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
